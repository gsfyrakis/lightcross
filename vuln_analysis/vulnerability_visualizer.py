import numpy as np
import pandas as pd
import csv
from vulnerability_metrics import calculate_vulnerability_metrics
# Set the matplotlib backend to 'Agg' before importing plt
# This uses a non-interactive backend that doesn't require tkinter
import matplotlib

matplotlib.use('Agg')
import matplotlib.pyplot as plt

def visualize_metrics(metrics):
    """
    Visualize the vulnerability metrics using matplotlib.
    
    Args:
        metrics: Dictionary containing the calculated metrics
    """
    # Create a dataframe for overall and per-category metrics
    categories = ['Overall'] + list(metrics['per_category'].keys())
    
    # Extract metrics
    recall_values = [metrics['overall']['recall']] + [metrics['per_category'][cat]['recall'] for cat in metrics['per_category']]
    precision_values = [metrics['overall']['precision']] + [metrics['per_category'][cat]['precision'] for cat in metrics['per_category']]
    f1_values = [metrics['overall']['f1_score']] + [metrics['per_category'][cat]['f1_score'] for cat in metrics['per_category']]
    
    tp_values = [metrics['overall']['true_positives']] + [metrics['per_category'][cat]['true_positives'] for cat in metrics['per_category']]
    fp_values = [metrics['overall']['false_positives']] + [metrics['per_category'][cat]['false_positives'] for cat in metrics['per_category']]
    fn_values = [metrics['overall']['false_negatives']] + [metrics['per_category'][cat]['false_negatives'] for cat in metrics['per_category']]
    
    # Create a figure with multiple subplots
    fig = plt.figure(figsize=(15, 12))
    fig.suptitle('Smart Contract Vulnerability Detection Metrics', fontsize=16)
    
    # Plot 1: Recall, Precision, F1 Score
    ax1 = fig.add_subplot(2, 2, 1)
    x = np.arange(len(categories))
    width = 0.25
    
    ax1.bar(x - width, recall_values, width, label='Recall', color='blue', alpha=0.7)
    ax1.bar(x, precision_values, width, label='Precision', color='green', alpha=0.7)
    ax1.bar(x + width, f1_values, width, label='F1 Score', color='red', alpha=0.7)
    
    ax1.set_ylabel('Score')
    ax1.set_title('Metrics by Category')
    ax1.set_xticks(x)
    ax1.set_xticklabels(categories, rotation=45, ha='right')
    ax1.legend()
    ax1.set_ylim(0, 1)
    
    # Plot 2: TP, FP, FN
    ax2 = fig.add_subplot(2, 2, 2)
    
    ax2.bar(x - width, tp_values, width, label='True Positives', color='blue', alpha=0.7)
    ax2.bar(x, fp_values, width, label='False Positives', color='red', alpha=0.7)
    ax2.bar(x + width, fn_values, width, label='False Negatives', color='orange', alpha=0.7)
    
    ax2.set_ylabel('Count')
    ax2.set_title('TP, FP, FN by Category')
    ax2.set_xticks(x)
    ax2.set_xticklabels(categories, rotation=45, ha='right')
    ax2.legend()
    
    # Plot 3: Heatmap of precision vs recall
    ax3 = fig.add_subplot(2, 2, 3)
    data = pd.DataFrame({
        'Category': categories,
        'Recall': recall_values,
        'Precision': precision_values
    })
    
    # Sort by F1 score
    data['F1'] = f1_values
    data = data.sort_values('F1', ascending=False)
    
    ax3.scatter(data['Recall'], data['Precision'], s=100, alpha=0.7)
    
    # Add category labels to points
    for i, txt in enumerate(data['Category']):
        ax3.annotate(txt, (data['Recall'].iloc[i], data['Precision'].iloc[i]), 
                    xytext=(5, 5), textcoords='offset points')
    
    ax3.set_xlabel('Recall')
    ax3.set_ylabel('Precision')
    ax3.set_title('Precision vs Recall by Category')
    ax3.grid(True, linestyle='--', alpha=0.7)
    ax3.set_xlim(0, 1)
    ax3.set_ylim(0, 1)
    
    # Plot 4: Stacked bar chart of TP, FP, FN proportions
    ax4 = fig.add_subplot(2, 2, 4)
    
    # Calculate proportions
    total_values = [tp + fp + fn for tp, fp, fn in zip(tp_values, fp_values, fn_values)]
    tp_prop = [tp / total if total > 0 else 0 for tp, total in zip(tp_values, total_values)]
    fp_prop = [fp / total if total > 0 else 0 for fp, total in zip(fp_values, total_values)]
    fn_prop = [fn / total if total > 0 else 0 for fn, total in zip(fn_values, total_values)]
    
    ax4.bar(x, tp_prop, label='True Positives', color='blue', alpha=0.7)
    ax4.bar(x, fp_prop, bottom=tp_prop, label='False Positives', color='red', alpha=0.7)
    ax4.bar(x, fn_prop, bottom=[sum(x) for x in zip(tp_prop, fp_prop)], label='False Negatives', color='orange', alpha=0.7)
    
    ax4.set_ylabel('Proportion')
    ax4.set_title('Proportions of TP, FP, FN by Category')
    ax4.set_xticks(x)
    ax4.set_xticklabels(categories, rotation=45, ha='right')
    ax4.legend()
    ax4.set_ylim(0, 1)
    
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig('vulnerability_metrics.png', dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """Main function to visualize vulnerability metrics."""
    ground_truth_file = 'sb_vulnerabilities_dasp.csv'
    
    output_files = [
        'output_access_control.csv',
        'output_arithmetic.csv',
        'output_bad_randomness.csv',
        'output_denial_of_service.csv',
        'output_front_running.csv',
        'output_other.csv',
        'output_reentrancy.csv',
        'output_short_addresses.csv',
        'output_time_manipulation.csv',
        'output_unchecked_low_level_calls.csv'
    ]
    
    metrics = calculate_vulnerability_metrics(ground_truth_file, output_files)
    visualize_metrics(metrics)
    print("Visualization saved to vulnerability_metrics.png")


if __name__ == "__main__":
    main()
