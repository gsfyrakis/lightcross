#!/usr/bin/env python3
"""
Smart Contract Vulnerability Analyzer

This script analyzes vulnerability detection results from various tools,
computes metrics, generates visualizations, and produces a comprehensive report.

Usage:
    python vulnerability_analyzer.py

Requirements:
    - Python 3.6+
    - pandas
    - matplotlib
    - numpy
"""

import os
import argparse
import sys
from vulnerability_metrics import calculate_vulnerability_metrics
from vulnerability_visualizer import visualize_metrics
from vulnerability_report import generate_comprehensive_report


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Smart Contract Vulnerability Analyzer')
    
    parser.add_argument('--ground-truth', type=str, default='sb_vulnerabilities_dasp.csv',
                        help='Path to the ground truth CSV file (default: sb_vulnerabilities_dasp.csv)')
    
    parser.add_argument('--output-dir', type=str, default='.',
                        help='Directory containing output CSV files (default: current directory)')
    
    parser.add_argument('--report-file', type=str, default='vulnerability_report.txt',
                        help='Path to save the report (default: vulnerability_report.txt)')
    
    parser.add_argument('--plot-file', type=str, default='vulnerability_metrics.png',
                        help='Path to save the visualization (default: vulnerability_metrics.png)')
    
    parser.add_argument('--metrics-file', type=str, default='vulnerability_metrics.csv',
                        help='Path to save the metrics CSV (default: vulnerability_metrics.csv)')
    
    parser.add_argument('--no-visualization', action='store_true',
                        help='Skip generating visualizations')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Enable verbose output')
    
    return parser.parse_args()


def find_output_files(output_dir):
    """Find all vulnerability output CSV files in the specified directory."""
    output_files = []
    
    # Expected output files
    expected_files = [
        'output_access_control.csv',
        'output_arithmetic.csv',
        'output_bad_randomness.csv',
        'output_denial_of_service.csv',
        'output_front_running.csv',
        'output_other.csv',
        'output_reentrancy.csv',
        'output_short_addresses.csv',
        'output_time_manipulation.csv',
        'output_unchecked_low_level_calls.csv'
    ]
    
    # Check for expected files
    for filename in expected_files:
        file_path = os.path.join(output_dir, filename)
        if os.path.isfile(file_path):
            output_files.append(file_path)
    
    return output_files


def main():
    """Main function to run the vulnerability analysis."""
    # Parse command line arguments
    args = parse_arguments()
    
    # Verify ground truth file exists
    if not os.path.isfile(args.ground_truth):
        print(f"Error: Ground truth file '{args.ground_truth}' not found")
        sys.exit(1)
    
    # Find output files
    output_files = find_output_files(args.output_dir)
    
    if not output_files:
        print(f"Error: No output files found in directory '{args.output_dir}'")
        sys.exit(1)
    
    if args.verbose:
        print(f"Found {len(output_files)} output files:")
        for file in output_files:
            print(f"  - {file}")
    
    # Calculate metrics
    print("Calculating vulnerability metrics...")
    metrics = calculate_vulnerability_metrics(args.ground_truth, output_files)
    
    # Generate report
    print("Generating comprehensive report...")
    with open(args.report_file, 'w') as f:
        # Redirect stdout to the file
        original_stdout = sys.stdout
        sys.stdout = f
        generate_comprehensive_report(metrics)
        sys.stdout = original_stdout
    
    print(f"Report saved to {args.report_file}")
    
    # Generate visualization if not disabled
    if not args.no_visualization:
        print("Generating visualizations...")
        visualize_metrics(metrics)
        print(f"Visualization saved to {args.plot_file}")
    
    # Print summary
    print("\nSummary of Results:")
    print(f"True Positives: {metrics['overall']['true_positives']}")
    print(f"False Positives: {metrics['overall']['false_positives']}")
    print(f"False Negatives: {metrics['overall']['false_negatives']}")
    print(f"Recall: {metrics['overall']['recall']:.4f}")
    print(f"Precision: {metrics['overall']['precision']:.4f}")
    print(f"F1 Score: {metrics['overall']['f1_score']:.4f}")
    
    # Print top and bottom categories by F1 score
    print("\nTop Performing Categories:")
    top_categories = sorted(
        metrics['per_category'].items(),
        key=lambda x: x[1]['f1_score'],
        reverse=True
    )[:3]
    
    for category, cat_metrics in top_categories:
        print(f"  {category}: F1={cat_metrics['f1_score']:.4f}")
    
    print("\nUnderperforming Categories:")
    bottom_categories = sorted(
        [(c, m) for c, m in metrics['per_category'].items() if m['ground_truth_files'] > 0],
        key=lambda x: x[1]['f1_score']
    )[:3]
    
    for category, cat_metrics in bottom_categories:
        print(f"  {category}: F1={cat_metrics['f1_score']:.4f}")
    
    print(f"\nFor more details, see the full report in {args.report_file}")


if __name__ == "__main__":
    main()