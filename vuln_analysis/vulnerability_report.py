import csv
import os
import pandas as pd
import matplotlib.pyplot as plt
from collections import defaultdict
from vulnerability_metrics import calculate_vulnerability_metrics


def generate_comprehensive_report(metrics):
    """
    Generate a comprehensive report of vulnerability metrics.
    
    Args:
        metrics: Dictionary containing the calculated metrics
    """
    print("\n" + "="*80)
    print(" "*30 + "VULNERABILITY ANALYSIS REPORT")
    print("="*80)
    
    # Print overall metrics
    print("\n1. OVERALL METRICS")
    print("-----------------")
    print(f"True Positives (TP): {metrics['overall']['true_positives']}")
    print(f"False Positives (FP): {metrics['overall']['false_positives']}")
    print(f"False Negatives (FN): {metrics['overall']['false_negatives']}")
    print(f"Recall: {metrics['overall']['recall']:.4f}")
    print(f"Precision: {metrics['overall']['precision']:.4f}")
    print(f"F1 Score: {metrics['overall']['f1_score']:.4f}")
    
    # Interpretation of overall metrics
    print("\nInterpretation:")
    if metrics['overall']['precision'] < 0.3:
        print("- LOW PRECISION: The tools generate a high number of false alarms.")
    elif metrics['overall']['precision'] < 0.7:
        print("- MODERATE PRECISION: The tools have a fair balance of true and false detections.")
    else:
        print("- HIGH PRECISION: The tools accurately identify vulnerabilities with few false alarms.")
    
    if metrics['overall']['recall'] < 0.3:
        print("- LOW RECALL: The tools miss many actual vulnerabilities.")
    elif metrics['overall']['recall'] < 0.7:
        print("- MODERATE RECALL: The tools detect a fair portion of actual vulnerabilities.")
    else:
        print("- HIGH RECALL: The tools successfully detect most actual vulnerabilities.")
    
    # Print metrics per category
    print("\n2. METRICS BY VULNERABILITY CATEGORY")
    print("-----------------------------------")
    
    # Sort categories by F1 score for better presentation
    sorted_categories = sorted(
        metrics['per_category'].items(),
        key=lambda x: x[1]['f1_score'],
        reverse=True
    )
    
    for category, cat_metrics in sorted_categories:
        print(f"\n{category}:")
        print(f"  Ground Truth Files: {cat_metrics['ground_truth_files']}")
        print(f"  True Positives: {cat_metrics['true_positives']}")
        print(f"  False Positives: {cat_metrics['false_positives']}")
        print(f"  False Negatives: {cat_metrics['false_negatives']}")
        print(f"  Recall: {cat_metrics['recall']:.4f}")
        print(f"  Precision: {cat_metrics['precision']:.4f}")
        print(f"  F1 Score: {cat_metrics['f1_score']:.4f}")
    
    # Best and worst performing categories
    print("\n3. PERFORMANCE ANALYSIS")
    print("----------------------")
    
    # Best categories (by F1 score)
    best_categories = sorted(
        metrics['per_category'].items(),
        key=lambda x: x[1]['f1_score'],
        reverse=True
    )[:3]
    
    print("\nTop Performing Categories (by F1 Score):")
    for i, (category, cat_metrics) in enumerate(best_categories, 1):
        print(f"{i}. {category}: F1={cat_metrics['f1_score']:.4f}, "
              f"Precision={cat_metrics['precision']:.4f}, "
              f"Recall={cat_metrics['recall']:.4f}")
    
    # Worst categories (by F1 score, but with at least some ground truth files)
    worst_categories = sorted(
        [(c, m) for c, m in metrics['per_category'].items() if m['ground_truth_files'] > 0],
        key=lambda x: x[1]['f1_score']
    )[:3]
    
    print("\nUnderperforming Categories (by F1 Score):")
    for i, (category, cat_metrics) in enumerate(worst_categories, 1):
        print(f"{i}. {category}: F1={cat_metrics['f1_score']:.4f}, "
              f"Precision={cat_metrics['precision']:.4f}, "
              f"Recall={cat_metrics['recall']:.4f}")
    
    # Analysis of false positives
    fp_heavy_categories = sorted(
        metrics['per_category'].items(),
        key=lambda x: x[1]['false_positives'],
        reverse=True
    )[:3]
    
    print("\nCategories with Most False Positives:")
    for i, (category, cat_metrics) in enumerate(fp_heavy_categories, 1):
        print(f"{i}. {category}: FP={cat_metrics['false_positives']}, "
              f"Precision={cat_metrics['precision']:.4f}")
    
    # Categories with significant false negatives
    fn_heavy_categories = sorted(
        [(c, m) for c, m in metrics['per_category'].items() if m['ground_truth_files'] > 0],
        key=lambda x: x[1]['false_negatives'],
        reverse=True
    )[:3]
    
    print("\nCategories with Most False Negatives:")
    for i, (category, cat_metrics) in enumerate(fn_heavy_categories, 1):
        print(f"{i}. {category}: FN={cat_metrics['false_negatives']}, "
              f"Recall={cat_metrics['recall']:.4f}")
    
    # Calculate the balanced accuracy
    balanced_accuracy = (metrics['overall']['recall'] + (metrics['overall']['true_positives'] / 
                        (metrics['overall']['true_positives'] + metrics['overall']['false_positives']) 
                        if (metrics['overall']['true_positives'] + metrics['overall']['false_positives']) > 0 
                        else 0)) / 2
    
    print("\n4. ADDITIONAL METRICS")
    print("--------------------")
    print(f"Balanced Accuracy: {balanced_accuracy:.4f}")
    
    # False discovery rate
    fdr = (metrics['overall']['false_positives'] / 
          (metrics['overall']['true_positives'] + metrics['overall']['false_positives'])
          if (metrics['overall']['true_positives'] + metrics['overall']['false_positives']) > 0 
          else 0)
    print(f"False Discovery Rate: {fdr:.4f}")
    
    # False omission rate
    miss_rate = (metrics['overall']['false_negatives'] / 
                (metrics['overall']['false_negatives'] + metrics['overall']['true_positives'])
                if (metrics['overall']['false_negatives'] + metrics['overall']['true_positives']) > 0 
                else 0)
    print(f"Miss Rate: {miss_rate:.4f}")
    
    print("\n5. CONCLUSIONS AND RECOMMENDATIONS")
    print("----------------------------------")
    
    # General assessment
    if metrics['overall']['f1_score'] < 0.3:
        print("Overall, the vulnerability detection tools show POOR performance.")
    elif metrics['overall']['f1_score'] < 0.6:
        print("Overall, the vulnerability detection tools show MODERATE performance.")
    else:
        print("Overall, the vulnerability detection tools show GOOD performance.")
    
    # Recommendations based on analysis
    print("\nRecommendations:")
    
    # If precision is low but recall is high
    if metrics['overall']['precision'] < 0.3 and metrics['overall']['recall'] > 0.6:
        print("- Improve PRECISION by refining detection algorithms to reduce false alarms.")
        print("- Consider implementing better heuristics for the categories with high false positives.")
    
    # If recall is low but precision is high
    elif metrics['overall']['recall'] < 0.3 and metrics['overall']['precision'] > 0.6:
        print("- Improve RECALL by enhancing detection coverage for missed vulnerabilities.")
        print("- Focus on the categories with high false negatives.")
    
    # If both precision and recall are low
    elif metrics['overall']['precision'] < 0.3 and metrics['overall']['recall'] < 0.3:
        print("- Major improvements needed in both precision and recall.")
        print("- Consider using multiple complementary detection tools.")
        print("- Revise detection algorithms across all categories.")
    
    # Specific category recommendations
    print("\nCategory-specific recommendations:")
    
    for category, cat_metrics in worst_categories:
        print(f"- For {category} vulnerabilities:")
        if cat_metrics['precision'] < 0.2:
            print(f"  * Reduce false positives with better pattern matching.")
        if cat_metrics['recall'] < 0.2:
            print(f"  * Improve detection methods to catch more actual vulnerabilities.")
    
    print("\n" + "="*80)


def main():
    """Main function to generate the comprehensive report."""
    ground_truth_file = 'sbvulnerabilitiesdasp.csv'
    
    output_files = [
        'output_access_control.csv',
        'output_arithmetic.csv',
        'output_bad_randomness.csv',
        'output_denial_of_service.csv',
        'output_front_running.csv',
        'output_other.csv',
        'output_reentrancy.csv',
        'output_short_addresses.csv',
        'output_time_manipulation.csv',
        'output_unchecked_low_level_calls.csv'
    ]
    
    metrics = calculate_vulnerability_metrics(ground_truth_file, output_files)
    generate_comprehensive_report(metrics)
    
    # Save the report to a text file
    with open('vulnerability_report.txt', 'w') as f:
        # Redirect stdout to the file
        import sys
        original_stdout = sys.stdout
        sys.stdout = f
        
        generate_comprehensive_report(metrics)
        
        # Reset stdout
        sys.stdout = original_stdout
    
    print("\nComprehensive report saved to vulnerability_report.txt")


if __name__ == "__main__":
    main()
