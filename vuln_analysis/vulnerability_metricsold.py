import os
import pandas as pd
import re
from collections import defaultdict

# Define the mapping between output categories and DASP categories
CATEGORY_MAPPING = {
    'access_control': 'Access Control',
    'arithmetic': 'Arithmetic Issues',
    'bad_randomness': 'Bad Randomness',
    'denial_of_service': 'Denial of Service',
    'front_running': 'Front-Running',
    'reentrancy': 'Reentrancy',
    'short_addresses': 'Short Address/Parameter Attack',
    'time_manipulation': 'Time Manipulation',
    'unchecked_low_level_calls': 'Unchecked Return Values',
    'other': 'Unknown Unknowns'
}

class VulnerabilityAnalyzer:
    def __init__(self, output_files_dir='.', mapping_file='sb_vulnerabilities_dasp.csv'):
        self.output_files_dir = output_files_dir
        self.mapping_file = mapping_file
        self.output_files = [f for f in os.listdir(output_files_dir) if f.startswith('output_') and f.endswith('.csv')]
        self.metrics = {}
        self.load_mapping_data()
        
    def load_mapping_data(self):
        """Load the ground truth mapping data"""
        self.mapping_df = pd.read_csv(self.mapping_file)
        
        # Create lookup structures
        self.vulnerable_files = defaultdict(list)
        
        for _, row in self.mapping_df.iterrows():
            # Normalize path for comparison
            path = self.normalize_path(row['path'])
            dasp_category = row['dasp']
            
            self.vulnerable_files[path].append(dasp_category)
    
    def normalize_path(self, path):
        """Normalize file paths for comparison"""
        # Remove prefixes like 'dataset/' or 'smartbugs-dataset/'
        normalized = re.sub(r'^(dataset/|smartbugs-dataset/)', '', path)
        
        # Remove line numbers if present (e.g., #L10-L20)
        normalized = re.sub(r'#L\d+(-L\d+)?$', '', normalized)
        
        return normalized
    
    def analyze_vulnerabilities(self):
        """Analyze all vulnerability reports and calculate metrics"""
        # Initialize metrics for each category
        for category in CATEGORY_MAPPING.values():
            self.metrics[category] = {
                'TP': 0,  # True Positives
                'FP': 0,  # False Positives
                'FN': 0,  # False Negatives
                'total_detected': 0,  # Total detections by tools
                'total_actual': 0,    # Total actual vulnerabilities
            }
        
        # Count total actual vulnerabilities by category
        for vulnerabilities in self.vulnerable_files.values():
            for category in vulnerabilities:
                self.metrics[category]['total_actual'] += 1
        
        # Process each output file
        for output_file in self.output_files:
            # Extract category from filename
            category_key = output_file.replace('output_', '').replace('.csv', '')
            if category_key in CATEGORY_MAPPING:
                dasp_category = CATEGORY_MAPPING[category_key]
                
                # Read the file
                df = pd.read_csv(os.path.join(self.output_files_dir, output_file))
                
                # Process each vulnerability report
                for _, row in df.iterrows():
                    file_path = self.normalize_path(row['File'])
                    self.metrics[dasp_category]['total_detected'] += 1
                    
                    # Check if this file actually has this vulnerability type
                    if file_path in self.vulnerable_files:
                        if dasp_category in self.vulnerable_files[file_path]:
                            self.metrics[dasp_category]['TP'] += 1
                        else:
                            self.metrics[dasp_category]['FP'] += 1
                    else:
                        self.metrics[dasp_category]['FP'] += 1
        
        # Calculate false negatives
        for category in self.metrics:
            self.metrics[category]['FN'] = (
                self.metrics[category]['total_actual'] - 
                self.metrics[category]['TP']
            )
            
            # Calculate precision and recall
            if self.metrics[category]['TP'] + self.metrics[category]['FP'] > 0:
                self.metrics[category]['precision'] = (
                    self.metrics[category]['TP'] / 
                    (self.metrics[category]['TP'] + self.metrics[category]['FP'])
                )
            else:
                self.metrics[category]['precision'] = 0
                
            if self.metrics[category]['TP'] + self.metrics[category]['FN'] > 0:
                self.metrics[category]['recall'] = (
                    self.metrics[category]['TP'] / 
                    (self.metrics[category]['TP'] + self.metrics[category]['FN'])
                )
            else:
                self.metrics[category]['recall'] = 0
        
        # Calculate overall metrics
        total_tp = sum(m['TP'] for m in self.metrics.values())
        total_fp = sum(m['FP'] for m in self.metrics.values())
        total_fn = sum(m['FN'] for m in self.metrics.values())
        
        if total_tp + total_fp > 0:
            overall_precision = total_tp / (total_tp + total_fp)
        else:
            overall_precision = 0
            
        if total_tp + total_fn > 0:
            overall_recall = total_tp / (total_tp + total_fn)
        else:
            overall_recall = 0
            
        self.metrics['Overall'] = {
            'TP': total_tp,
            'FP': total_fp,
            'FN': total_fn,
            'precision': overall_precision,
            'recall': overall_recall
        }
    
    def print_results(self):
        """Print the analysis results in a formatted table"""
        print(f"{'Category':<30} {'TP':<8} {'FP':<8} {'FN':<8} {'Precision':<10} {'Recall':<10}")
        print("-" * 80)
        
        for category, metrics in sorted(self.metrics.items()):
            precision = metrics.get('precision', 0)
            recall = metrics.get('recall', 0)
            
            print(f"{category:<30} {metrics['TP']:<8} {metrics['FP']:<8} {metrics['FN']:<8} "
                  f"{precision:.4f}   {recall:.4f}")
    
    def export_results(self, output_file='vulnerability_metrics.csv'):
        """Export the results to a CSV file"""
        results = []
        
        for category, metrics in self.metrics.items():
            results.append({
                'Category': category,
                'True Positives': metrics['TP'],
                'False Positives': metrics['FP'],
                'False Negatives': metrics['FN'],
                'Precision': metrics.get('precision', 0),
                'Recall': metrics.get('recall', 0)
            })
        
        pd.DataFrame(results).to_csv(output_file, index=False)
        print(f"Results exported to {output_file}")


# Main execution
if __name__ == "__main__":
    analyzer = VulnerabilityAnalyzer()
    analyzer.analyze_vulnerabilities()
    analyzer.print_results()
    analyzer.export_results()
